# -*- coding: utf-8 -*-
"""Project_1_ecommerce objects identifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UKtl-MEEppyTQhGIqPl6JWG1TH7qEGEd
"""

pip install tensorflow

from __future__ import absolute_import, division, print_function, unicode_literals

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import clear_output
from six.moves import urllib

import tensorflow as tf
print(tf.version)
#now lets create some tensorflow variables having zero rank. By zero rank we mean that they have scaler values, the values dont change, hence zero rank
string = tf.Variable("this is a string", tf.string)
number1 = tf.Variable(23 , tf.int16)
float1 = tf.Variable(23.231, tf.float64)
# print(string)

#now lets create rank 1 and rank 2 tensors
rank1_tensor = tf.Variable(["hello" , "world"], tf.string)
rank2_tensor = tf.Variable([["hello", "there"] ,["new" , "world"]], tf.string)
# print(rank2_tensor)
# tf.rank(rank2_tensor)
# this is a rank 2 because, it is a 2 level nested list
# tensor1 = tf.ones([1,2,3])
# print(tensor1)
# tensor2 = tf.reshape(tensor1, [2,3,1])
# print(tensor2)
# tensor3 = tf.reshape(tensor1, [3,-1])
# print(tensor3)
# tensor4 = tf.ones([5,5,5,5,5])
# print(tensor4)

#loading the dataset
dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')
dfeval = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv')
#print(dftrain.head())
y_train = dftrain.pop('survived')
y_eval = dfeval.pop('survived')
# print(dftrain.head())
#how to print one particular row
# print(dftrain.loc[0])
# print(y_train.loc[0])
#thats how you access rows of a pandas dataframe
print(dftrain["sex"])
#and thats how we access columns of a dataframe

dftrain.head()

dftrain.describe( )

dftrain.shape

dftrain.age.hist(bins = 20)

dftrain['sex'].value_counts().plot(kind='barh')

dftrain['class'].value_counts().plot(kind = 'barh')

pd.concat([dftrain, y_train], axis = 1).groupby('sex').survived.mean().plot(kind = 'barh').set_xlabel('% survive')

#we can draw out various conclusions from this study above

dfeval.shape
dfeval.head( )

CATEGORICAL_COLUMNS = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck', 'embark_town', 'alone']
NUMERIC_COLUMNS = ['age', 'fare']

feature_columns = []
for feature_name in CATEGORICAL_COLUMNS:
  vocabulary = dftrain[feature_name].unique()
  feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))
# print(vocabulary)

#our model needs to know what are the various categorical columns and what are the unique values associated with that column

for feature_name in NUMERIC_COLUMNS:
  feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype = tf.float32))

print(feature_columns)

dftrain['sex'].unique()
#this function provides us unique categorical values from a particular column of a dataframe

def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):
  def input_function():  # inner function, this will be returned
    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))  # create tf.data.Dataset object with data and its label
    if shuffle:
      ds = ds.shuffle(1000)  # randomize order of data
    ds = ds.batch(batch_size).repeat(num_epochs)  # split dataset into batches of 32 and repeat process for number of epochs
    return ds  # return a batch of the dataset
  return input_function  # return a function object for use

train_input_fn = make_input_fn(dftrain, y_train)  # here we will call the input_function that was returned to us to get a dataset object we can feed to the model
eval_input_fn = make_input_fn(dfeval, y_eval, num_epochs=1, shuffle=False)

linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)

linear_est.train(train_input_fn)  # train
result = list(linear_est.evaluate(eval_input_fn))  # get model metrics/stats by testing on tetsing data

clear_output()  # clears consoke output
  # the result variable is simply a dict of stats about our model
print(result)

print(dfeval.loc[2])
print(y_eval.loc[2])

#now we are going to learn classification

CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']
SPECIES = ['Setosa', 'Versicolor', 'Virginica']

train_path = tf.keras.utils.get_file(
    "iris_training.csv", "https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv")
test_path = tf.keras.utils.get_file(
    "iris_test.csv", "https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv")

train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)
test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)

# train.head()
test.head()

train_y = train.pop('Species')
test_y = test.pop('Species')
train.head()

train.shape #120 entries with 4 features

train_y.head()

def input_fn(features, labels, training=True, batch_size=256):
    # Convert the inputs to a Dataset.
    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))

    # Shuffle and repeat if you are in training mode.
    if training:
        dataset = dataset.shuffle(1000).repeat()

    return dataset.batch(batch_size)

my_feature_columns = []
for key in train.keys():
    my_feature_columns.append(tf.feature_column.numeric_column(key=key))
print(my_feature_columns)

# Build a DNN with 2 hidden layers with 30 and 10 hidden nodes each.
classifier = tf.estimator.DNNClassifier(
    feature_columns=my_feature_columns,
    # Two hidden layers of 30 and 10 nodes respectively.
    hidden_units=[30, 10],
    # The model must choose between 3 classes.
    n_classes=3)

classifier.train(
    input_fn=lambda: input_fn(train, train_y, training=True),
    steps=5000)
# We include a lambda to avoid creating an inner function previously

eval_result = classifier.evaluate(input_fn=lambda: input_fn(test, test_y, training=False))

print('\nTest set accuracy: {accuracy:0.3f}\n'.format(**eval_result))

# def input_fn(features, batch_size=256):
#     # Convert the inputs to a Dataset without labels.
#     return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)

# features = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']
# predict = {}

# print("Please type numeric values as prompted.")
# for feature in features:
#   valid = True
#   while valid:
#     val = input(feature + ": ")
#     if not val.isdigit(): valid = False

#   predict[feature] = [float(val)]

# predictions = classifier.predict(input_fn=lambda: input_fn(predict))
# for pred_dict in predictions:
#     class_id = pred_dict['class_ids'][0]
#     probability = pred_dict['probabilities'][class_id]

#     print('Prediction is "{}" ({:.1f}%)'.format(
#         SPECIES[class_id], 100 * probability))

import tensorflow_probability as tfp  # We are using a different module from tensorflow this time
import tensorflow as tf

tfd = tfp.distributions  # making a shortcut for later on
initial_distribution = tfd.Categorical(probs=[0.8, 0.2])  # Refer to point 2 above
transition_distribution = tfd.Categorical(probs=[[0.5, 0.5],
                                                 [0.2, 0.8]])  # refer to points 3 and 4 above
observation_distribution = tfd.Normal(loc=[0., 15.], scale=[5., 10.])  # refer to point 5 above
#here loc refers to mean values and
model = tfd.HiddenMarkovModel(
    initial_distribution=initial_distribution,
    transition_distribution=transition_distribution,
    observation_distribution=observation_distribution,
    num_steps=7)

# mean = model.mean()

# # due to the way TensorFlow works on a lower level we need to evaluate part of the graph
# # from within a session to see the value of this tensor

# # in the new version of tensorflow we need to use tf.compat.v1.Session() rather than just tf.Session()
# with tf.compat.v1.Session() as sess:
#   print(mean.numpy())

#   #the output is the temperature predictions for next few days

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
# TensorFlow and tf.keras
import tensorflow as tf
from tensorflow import keras

# Helper libraries
import numpy as np
import matplotlib.pyplot as plt

fashion_mnist = keras.datasets.fashion_mnist  # load dataset

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()  # split into tetsing and training

train_images.shape

#this output just means that we have 60k images of 28x28 pixels

train_images[0,23,23]

train_labels[:10]

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

plt.figure()
plt.imshow(train_images[1])
plt.colorbar()
plt.grid(False)
plt.show()

train_images = train_images / 255.0

test_images = test_images / 255.0
#we are doing this thing to scale down calculations for our neural network
#since each pixel is within range 0 to 255, dividing by 255 will scale down the whole system for us

#we are going to use keras sequential model here, the information flows from left side to right side, starting from source layers, moving into the hiddne layers
#and then the final layer
model = keras.Sequential([
    keras.layers.Flatten(input_shape=(28, 28)),  # input layer (1) , converts matrix of pixels into a stream of neurons, 784 neurons to be precise in this case
    keras.layers.Dense(128, activation='relu'),  # hidden layer (2), dense layer, previous layer neurons connected to all neurons of next layer.
    keras.layers.Dense(10, activation='softmax') # output layer (3), we have 10 as first parameter because we have 10 output classes, see above, we have 10 types of clothing articles and softmax will distribute this probability distribution between these 10 classes.
])

#we have now defined what our model looks like, how many layers are there, how many neurons are there in each layer. and next, we need to compile our model

# now we will compile the model, here adam and other function names are just functions that we have selected to do our gradient descent and loss factor calculations

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])
# so these parameters that we are giving this model, the oss function, the gradient descent function are called hyper-parameters
#we can change these hyper parameters according to our requirements

model.fit(train_images, train_labels, epochs=10)  # we pass the data, labels and epochs and watch the magic!
#epoch refers to one training attempt on our dataset

test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=1)

print('Test accuracy:', test_acc)
#verbose just means how much information its printing into the console
#now that we have trained the model, we have to test it on the testing data and thats what we're trying to do here

predictions = model.predict(test_images)

# test_images.shape
print(class_names[np.argmax(predictions[1])])
plt.figure()
plt.imshow(test_images[1])
plt.colorbar()
plt.grid(False)
plt.show()

COLOR = 'black'
plt.rcParams['text.color'] = COLOR
plt.rcParams['axes.labelcolor'] = COLOR

def predict(model, image, correct_label):
  class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
  prediction = model.predict(np.array([image]))
  predicted_class = class_names[np.argmax(prediction)]

  show_image(image, class_names[correct_label], predicted_class)


def show_image(img, label, guess):
  plt.figure()
  plt.imshow(img, cmap=plt.cm.binary)
  plt.title("Excpected: " + label)
  plt.xlabel("Guess: " + guess)
  plt.colorbar()
  plt.grid(False)
  plt.show()


def get_number():
  while True:
    num = input("Pick a number: ")
    if num.isdigit():
      num = int(num)
      if 0 <= num <= 1000:
        return int(num)
    else:
      print("Try again...")



num = get_number()
image = test_images[num]
label = test_labels[num]
predict(model, image, label)